name: CI - Syntax and Pipeline Checks

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

env:
  GRAFANA_PORT: 3000
  AIRFLOW_PORT: 8080
  CONTAINER_TIMEOUT: 300
  AIRFLOW_CONTAINER: airflow-webserver
  GRAFANA_CONTAINER: grafana

jobs:

  syntax_checks:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v5

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install YAML Validator
        run: pip install pyyaml

      - name: Check Python Syntax
        run: |
          for file in $(git ls-files '*.py'); do
            python -m py_compile "$file"
          done

      - name: Validate YAML Syntax
        run: |
          for file in $(git ls-files '*.yml' '*.yaml'); do
            python -c "import yaml; yaml.safe_load(open('$file'))"
          done


  trigger_pipeline:
    runs-on: ubuntu-latest
    needs: syntax_checks
    if: github.ref == 'refs/heads/development'

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v5

      # =========================
      # Create .env from Secrets
      # =========================
      - name: Create .env file
        run: |
          cat <<EOF > .env
          POSTGRES_USER=${{ secrets.POSTGRES_USER }}
          POSTGRES_PASSWORD=${{ secrets.POSTGRES_PASSWORD }}
          POSTGRES_DB=${{ secrets.POSTGRES_DB }}
          POSTGRES_PORT=5432

          AIRFLOW_EXECUTOR=LocalExecutor
          AIRFLOW_FERNET_KEY=${{ secrets.AIRFLOW_FERNET_KEY }}
          AIRFLOW_SECRET_KEY=${{ secrets.AIRFLOW_SECRET_KEY }}
          AIRFLOW_DAGS_PAUSED_AT_CREATION=true
          AIRFLOW_LOAD_EXAMPLES=false
          AIRFLOW_EXPOSE_CONFIG=true
          AIRFLOW_WEBSERVER_EXTERNAL_PORT=8087

          AIRFLOW_ADMIN_USER=admin
          AIRFLOW_ADMIN_PASSWORD=${{ secrets.AIRFLOW_ADMIN_PASSWORD }}
          AIRFLOW_ADMIN_FIRSTNAME=Admin
          AIRFLOW_ADMIN_LASTNAME=User
          AIRFLOW_ADMIN_EMAIL=admin@example.com

          ZOOKEEPER_CLIENT_PORT=2181
          ZOOKEEPER_TICK_TIME=2000
          ZOOKEEPER_SYNC_LIMIT=2

          KAFKA_BROKER_ID=1
          KAFKA_INTERNAL_PORT=9092
          KAFKA_EXTERNAL_PORT=9098
          KAFKA_REPLICATION_FACTOR=1
          KAFKA_LOG_RETENTION_HOURS=168
          KAFKA_LOG_SEGMENT_BYTES=1073741824
          KAFKA_UI_PORT=8081
          KAFKA_UI_EXTERNAL_PORT=8088

          SPARK_MASTER_PORT=7077
          SPARK_MASTER_WEBUI_PORT=8080
          SPARK_APP_UI_PORT=4040
          SPARK_WORKER_WEBUI_PORT_1=8083
          SPARK_WORKER_WEBUI_PORT_2=8084
          SPARK_WORKER_MEMORY=2g
          SPARK_WORKER_CORES=2
          SPARK_MASTER_CONTAINER=spark-master
          SPARK_MASTER_UI_EXTERNAL_PORT=8082

          KAFKA_TOPIC=earthquake_raw
          KAFKA_TOPIC_WOLF=wolf_seismic_stream

          SEISMIC_WS_URL=wss://www.seismicportal.eu/standing_order/websocket
          SEISMIC_WS_PING_INTERVAL=15
          SEISMIC_CSV_FILENAME=seismic_events.csv

          EARTHQUAKE_CHECKPOINT_PATH=/opt/delta-lake/checkpoints/earthquakes
          EARTHQUAKE_DELTA_TABLE_PATH=/opt/delta-lake/tables/earthquakes
          WOLF_CHECKPOINT_PATH=/opt/delta-lake/checkpoints/wolf_seismic
          WOLF_DELTA_TABLE_PATH=/opt/delta-lake/tables/wolf_seismic
          AIRPORT_DELTA_TABLE_PATH=/opt/delta-lake/tables/airports
          PROXIMITY_CHECKPOINT_PATH=/opt/delta-lake/checkpoints/proximity_events
          PROXIMITY_DELTA_TABLE_PATH=/opt/delta-lake/tables/proximity_events

          EARTHQUAKE_METRICS_PORT=8000
          WOLF_METRICS_PORT=8001
          AIRPORT_METRICS_PORT=8002
          EOF

      - name: Verify Docker
        run: |
          docker version
          docker compose version

      # Prepare Environment
      - name: Prepare .env File
        run: |
          if [ ! -f .env ]; then
            cp .env.example .env
          fi

      # Start Docker Compose services
      - name: Start Docker Compose
        run: |
          echo "Starting Docker Compose for Airflow + Spark + Kafka + Grafana..."
          docker compose -f docker-compose.yml up -d

      # Wait for Airflow to be ready
      - name: Wait for Services (Airflow & Grafana)
        run: |
          echo "Waiting for Airflow webserver to be ready..."
          timeout ${{ env.CONTAINER_TIMEOUT }} bash -c '
            until docker compose exec ${{ env.AIRFLOW_CONTAINER }} airflow version &> /dev/null; do
              echo "Airflow not ready yet, waiting 5 seconds..."
              sleep 5
            done
          ' || {
            echo "Airflow failed to start within ${{ env.CONTAINER_TIMEOUT }} seconds"
            docker compose logs ${{ env.AIRFLOW_CONTAINER }}
            exit 1
          }
          echo "Airflow is ready!"

          echo "Waiting for Grafana to be ready..."
          timeout ${{ env.CONTAINER_TIMEOUT }} bash -c '
            until curl -s http://localhost:${{ env.GRAFANA_PORT }}/api/health | grep -q "\"database\": \"ok\""; do
              echo "Grafana not ready yet, waiting 5 seconds..."
              sleep 5
            done
          ' || {
            echo "Grafana failed to start within ${{ env.CONTAINER_TIMEOUT }} seconds"
            docker compose logs ${{ env.GRAFANA_CONTAINER }}
            exit 1
          }
          echo "Grafana is ready!"

      # Validate Environment Dependencies
      - name: Validate Dependencies
        run: |
          chmod +x scripts/validate_env.sh
          ./scripts/validate_env.sh

      # Trigger Pipeline DAGs
      - name: Trigger DAGs
        run: |
          echo "Triggering DAGs..."
          chmod +x scripts/trigger_pipeline_dag.sh
          ./scripts/trigger_pipeline_dag.sh

      # Confirm Execution
      - name: Confirm DAG Trigger
        run: echo "All DAGs triggered successfully!"

      # Cleanup Docker
      - name: Cleanup Docker
        if: always()
        run: |
          echo "Cleaning up Docker containers and volumes..."
          docker compose -f docker-compose.yml down -v || true
